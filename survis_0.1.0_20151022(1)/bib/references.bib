

@INPROCEEDINGS{9884915,

  author={Pang, Lei and Zhang, Fengli and Li, Lu and Huang, Qiqi and Jiao, Yanan and Shao, Yun},

  booktitle={IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium}, 

  title={Assessing Buildings Damage from Multi-Temporal Sar Images Fusion using Semantic Change Detection}, 

  year={2022},

  volume={},

  number={},

  pages={6292-6295},

  keywords={Training;Open Access;Buildings;Semantics;Geoscience and remote sensing;Disaster management;Emergency services;SAR;Buildings damage;Semantic change detection;Siamese network},

  doi={10.1109/IGARSS46834.2022.9884915}}


@INPROCEEDINGS{10283446,
  author={Su, Jau-Lang and Yeh, Chia-Cheng and Alkhaleefah, Mohammad and Chang, Lena and Chang, Yang-Lang},
  booktitle={IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium}, 
  title={A Deep Convolutional Neural Network for Building Damage Evaluation from Satellite Images}, 
  year={2023},
  volume={},
  number={},
  pages={285-288},
  keywords={Deep learning;Training;Image segmentation;Architecture;Buildings;Feature extraction;Satellite images;Climate change;Deep learning;convolutional neural networks;telemetry satellite imagery;building segmentation;disaster damage assessment},
  doi={10.1109/IGARSS52108.2023.10283446}
}
@INPROCEEDINGS{9883139,

  author={Chen, Hongruixuan and Nemni, Edoardo and Vallecorsa, Sofia and Li, Xi and Wu, Chen and Bromley, Lars},

  booktitle={IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium}, 

  title={Dual-Tasks Siamese Transformer Framework for Building Damage Assessment}, 

  year={2022},

  volume={},

  number={},

  pages={1600-1603},

  keywords={Computer vision;Architecture;Buildings;Computer architecture;Transformers;Feature extraction;Decoding;Transformer;building damage assessment;multi-task learning;deep learning;multitemporal images},

  doi={10.1109/IGARSS46834.2022.9883139}
}

@INPROCEEDINGS{9412295,

  author={Gupta, Rohit and Shah, Mubarak},

  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 

  title={RescueNet: Joint Building Segmentation and Damage Assessment from Satellite Imagery}, 

  year={2021},

  volume={},

  number={},

  pages={4405-4411},

  keywords={Location awareness;Image segmentation;Computer vision;Satellites;Head;Architecture;Tiles},

  doi={10.1109/ICPR48806.2021.9412295}}

@article{Lee2020AssessingPD,
  title={Assessing Post-Disaster Damage from Satellite Imagery using Semi-Supervised Learning Techniques},
  author={Jihyeon Lee and Joseph Z. Xu and Kihyuk Sohn and Wenhan Lu and David Berthelot and Izzeddin Gur and Pranav Khaitan and Ke Huang and Kyriacos M. Koupparis and Bernhard Kowatsch},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.14004},
  url={https://api.semanticscholar.org/CorpusID:227227614}
}

@article{
doi:10.1073/pnas.2025400118,
author = {Hannes Mueller  and Andre Groeger  and Jonathan Hersh  and Andrea Matranga  and Joan Serrat },
title = {Monitoring war destruction from space using machine learning},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {23},
pages = {e2025400118},
year = {2021},
doi = {10.1073/pnas.2025400118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2025400118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2025400118},
abstract = {Existing data on building destruction in conflict zones rely on eyewitness reports or manual detection, which makes it generally scarce, incomplete, and potentially biased. This lack of reliable data imposes severe limitations for media reporting, humanitarian relief efforts, human-rights monitoring, reconstruction initiatives, and academic studies of violent conflict. This article introduces an automated method of measuring destruction in high-resolution satellite images using deep-learning techniques combined with label augmentation and spatial and temporal smoothing, which exploit the underlying spatial and temporal structure of destruction. As a proof of concept, we apply this method to the Syrian civil war and reconstruct the evolution of damage in major cities across the country. Our approach allows generating destruction data with unprecedented scope, resolution, and frequencyâ€”and makes use of the ever-higher frequency at which satellite imagery becomes available.}}


@article{Weber2020BuildingDD,
  title={Building Disaster Damage Assessment in Satellite Imagery with Multi-Temporal Fusion},
  author={Ethan Weber and Hassan Kan{\'e}},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.05525},
  url={https://api.semanticscholar.org/CorpusID:215745271}
}

@Article{rs13050905,
AUTHOR = {Wu, Chuyi and Zhang, Feng and Xia, Junshi and Xu, Yichen and Li, Guoqing and Xie, Jibo and Du, Zhenhong and Liu, Renyi},
TITLE = {Building Damage Detection Using U-Net with Attention Mechanism from Pre- and Post-Disaster Remote Sensing Datasets},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {905},
URL = {https://www.mdpi.com/2072-4292/13/5/905},
ISSN = {2072-4292},
ABSTRACT = {The building damage status is vital to plan rescue and reconstruction after a disaster and is also hard to detect and judge its level. Most existing studies focus on binary classification, and the attention of the model is distracted. In this study, we proposed a Siamese neural network that can localize and classify damaged buildings at one time. The main parts of this network are a variety of attention U-Nets using different backbones. The attention mechanism enables the network to pay more attention to the effective features and channels, so as to reduce the impact of useless features. We train them using the xBD dataset, which is a large-scale dataset for the advancement of building damage assessment, and compare their result balanced F (F1) scores. The score demonstrates that the performance of SEresNeXt with an attention mechanism gives the best performance among single models, with the F1 score reaching 0.787. To improve the accuracy, we fused the results and got the best overall F1 score of 0.792. To verify the transferability and robustness of the model, we selected the dataset on the Maxar Open Data Program of two recent disasters to investigate the performance. By visual comparison, the results show that our model is robust and transferable.},
DOI = {10.3390/rs13050905}
}

@article{Zhang2022SwinSUNetPT,
  title={SwinSUNet: Pure Transformer Network for Remote Sensing Image Change Detection},
  author={Cui Zhang and Liejun Wang and Shuli Cheng and Yongming Li},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  year={2022},
  volume={PP},
  pages={1-1},
  url={https://api.semanticscholar.org/CorpusID:247509609}
}

@ARTICLE{9442902,

  author={Shen, Yu and Zhu, Sijie and Yang, Taojiannan and Chen, Chen and Pan, Delu and Chen, Jianyu and Xiao, Liang and Du, Qian},

  journal={IEEE Transactions on Geoscience and Remote Sensing}, 

  title={BDANet: Multiscale Convolutional Neural Network With Cross-Directional Attention for Building Damage Assessment From Satellite Images}, 

  year={2022},

  volume={60},

  number={},

  pages={1-14},

  keywords={Buildings;Image segmentation;Satellites;Feature extraction;Task analysis;Neural networks;Remote sensing;Building damage assessment;convolutional neural network (CNN);cross-directional attention (CDA);CutMix;multiscale feature fusion (MFF);satellite image},

  doi={10.1109/TGRS.2021.3080580}}
